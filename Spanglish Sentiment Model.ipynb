{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyQNRixmKxDe"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets peft bitsandbytes accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaZTDj9OK7JM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, LlamaTokenizer, BitsAndBytesConfig, TrainingArguments, Trainer\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "from datasets import load_dataset, ClassLabel\n",
        "import numpy as np\n",
        "import json\n",
        "\n",
        "# Ensure we have a GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ugrws83aK_bW"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1) Load the raw JSON dataset\n",
        "raw = load_dataset(\"json\", data_files=\"spanglish_dataset.json\")[\"train\"]\n",
        "\n",
        "# 2) Convert the text labels into a ClassLabel feature\n",
        "#    This maps \"positive\",\"negative\",\"neutral\" â†’ 0,1,2 under the hood.\n",
        "raw = raw.cast_column(\n",
        "    \"sentiment_label\",\n",
        "    ClassLabel(names=[\"positive\", \"negative\", \"neutral\"])\n",
        ")\n",
        "\n",
        "# 3) Now you can stratify by that ClassLabel column\n",
        "split = raw.train_test_split(\n",
        "    test_size=0.2,\n",
        "    seed=42,\n",
        "    stratify_by_column=\"sentiment_label\"\n",
        ")\n",
        "train_dataset = split[\"train\"]\n",
        "test_dataset  = split[\"test\"]\n",
        "\n",
        "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
        "print(train_dataset.features)  # to verify sentiment_label is now ClassLabel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dp52BDK1LE3m"
      },
      "outputs": [],
      "source": [
        "# Specify the model name (a LLaMA-based 3B model from Hugging Face)\n",
        "model_name = \"openlm-research/open_llama_3b_v2\"\n",
        "\n",
        "# Load the tokenizer (use the LlamaTokenizer with use_fast=False as recommended for OpenLLaMA)\n",
        "tokenizer = LlamaTokenizer.from_pretrained(model_name, use_fast=False)\n",
        "# Use the end-of-sequence token as the padding token for batching\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Define the prompt template with proper formatting\n",
        "PROMPT_TEMPLATE_DETAILED = (\n",
        "    \"Perform sentiment analysis on this sentence, which may contain a mix of English and Spanish (Spanglish). Consider both languages and cultural context to determine sentiment. Choose from:\\n\"\n",
        "    \"{{\\\"positive\\\", \\\"negative\\\", \\\"neutral\\\"}}\\n\\n\"\n",
        "    \"Sentence: {sentence}\\n\\n\"\n",
        "    \"Respond in JSON format with:\\n\"\n",
        "    \"- \\\"prediction\\\": the predicted category\\n\"\n",
        "    \"- \\\"confidence\\\": your confidence score (0-1)\\n\"\n",
        "    \"- \\\"reason\\\": include any bilingual or cultural cues that influenced your decision\\n\\n\"\n",
        "    \"Example response:\\n\"\n",
        "    \"{{\\\"prediction\\\": \\\"positive\\\", \\\"confidence\\\": 0.92, \\\"reason\\\": \\\"The sentence contains affectionate language and positive slang common in both cultures\\\"}}\\n\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4Mtvr4RLGvm"
      },
      "outputs": [],
      "source": [
        "# Configure 8-bit loading\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "# Load the model in 8-bit mode\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
        "\n",
        "# Prepare the model for k-bit training (this hooks up grad support in int8 weights)\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Disable use_cache before enabling gradient checkpointing\n",
        "model.config.use_cache = False\n",
        "\n",
        "# Enable gradient checkpointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# Set up LoRA configuration\n",
        "lora_config = LoraConfig(\n",
        "    r=16,             # Rank of LoRA decomposition\n",
        "    lora_alpha=32,    # LoRA scaling factor\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\"],  # Target certain transformer sub-modules\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "# Add LoRA adapters to the model\n",
        "model = get_peft_model(model, lora_config)\n",
        "\n",
        "\n",
        "# Print trainable parameter information for verification\n",
        "trainable = [n for n, p in model.named_parameters() if p.requires_grad]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PaLFs9TtLIpN"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"spanglish_sentiment_model\",\n",
        "    overwrite_output_dir=True,\n",
        "    per_device_train_batch_size=4,\n",
        "    gradient_accumulation_steps=8,  # accumulate to simulate larger batch\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    logging_steps=50,\n",
        "    optim=\"adamw_torch\",\n",
        "    report_to=\"none\",        # disable hub logging\n",
        "    save_steps=5000,         # checkpoint every 5000 steps\n",
        "    save_total_limit=2       # only keep last 2 checkpoints\n",
        ")\n",
        "\n",
        "\n",
        "# Data collator to pad sequences and mask labels properly\n",
        "def data_collator(batch):\n",
        "    # Each item in batch already has input_ids and labels as lists\n",
        "    max_length = max(len(item[\"input_ids\"]) for item in batch)\n",
        "    input_ids_list = []\n",
        "    attention_mask_list = []\n",
        "    labels_list = []\n",
        "    for item in batch:\n",
        "        seq_len = len(item[\"input_ids\"])\n",
        "        pad_len = max_length - seq_len\n",
        "        # Pad input_ids with tokenizer.pad_token_id\n",
        "        input_ids_list.append(item[\"input_ids\"] + [tokenizer.pad_token_id] * pad_len)\n",
        "        # Pad attention mask with 0\n",
        "        attention_mask_list.append(item[\"attention_mask\"] + [0] * pad_len)\n",
        "        # Pad labels with -100 (so padded tokens are ignored in loss)\n",
        "        labels_list.append(item[\"labels\"] + [-100] * pad_len)\n",
        "    # Convert to tensors\n",
        "    input_ids = torch.tensor(input_ids_list, dtype=torch.long)\n",
        "    attention_mask = torch.tensor(attention_mask_list, dtype=torch.long)\n",
        "    labels = torch.tensor(labels_list, dtype=torch.long)\n",
        "    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QNTj0NsLRHnK"
      },
      "outputs": [],
      "source": [
        "# Train the model\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8wwyiwteHEJ"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/data.zip /content/spanglish_sentiment_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kPAlx3046Gn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "\n",
        "# 1) Use the exact model_name you set earlier:\n",
        "#    (e.g. \"openlm-research/open_llama_3b_v2\")\n",
        "print(\"Base model identifier:\", model_name)\n",
        "\n",
        "# 2) Configure 8-bit loading\n",
        "bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n",
        "\n",
        "# 3) Load the base LLaMA 3B model in 8-bit\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 4) Attach your fine-tuned adapters from disk\n",
        "model = PeftModel.from_pretrained(base_model, \"spanglish_sentiment_model\")\n",
        "\n",
        "# 5) Switch to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Quick check\n",
        "print(\"Model loaded with adapters. Trainable params:\")\n",
        "model.print_trainable_parameters()  # should list your LoRA parameters only\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AV3EC5G2LLDx"
      },
      "outputs": [],
      "source": [
        "import torch, time, re, json, ast\n",
        "from datasets import ClassLabel\n",
        "\n",
        "# 1) Ensure model is in eval mode on GPU\n",
        "print(\"Using device:\", next(model.parameters()).device)\n",
        "model.eval()\n",
        "\n",
        "# 2) Build integerâ†’string map if sentiment_label is a ClassLabel\n",
        "label_feat = split[\"test\"].features[\"sentiment_label\"]\n",
        "if isinstance(label_feat, ClassLabel):\n",
        "    int2str = {i: name for i, name in enumerate(label_feat.names)}\n",
        "else:\n",
        "    int2str = None\n",
        "\n",
        "# 3) Run evaluation\n",
        "correct = 0\n",
        "start = time.time()\n",
        "\n",
        "for idx, ex in enumerate(split[\"test\"]):\n",
        "    sentence   = ex[\"sentence\"]\n",
        "    true_label = ex[\"sentiment_label\"]\n",
        "\n",
        "    # a) Generate\n",
        "    prompt = PROMPT_TEMPLATE_DETAILED.format(sentence=sentence)\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(next(model.parameters()).device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(**inputs, max_new_tokens=50, do_sample=False, use_cache=True)\n",
        "\n",
        "    # b) Strip off prompt tokens & decode\n",
        "    gen_tokens = output[0, inputs[\"input_ids\"].shape[1]:]\n",
        "    txt = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    # c) Extract the last JSON block\n",
        "    matches = re.findall(r\"\\{.*?\\}\", txt, flags=re.DOTALL)\n",
        "    candidate = matches[-1] if matches else txt\n",
        "\n",
        "    # d) Parse prediction (try JSON, then ast.literal_eval)\n",
        "    raw = None\n",
        "    try:\n",
        "        data = json.loads(candidate)\n",
        "        raw = data.get(\"prediction\", None)\n",
        "    except json.JSONDecodeError:\n",
        "        try:\n",
        "            data = ast.literal_eval(candidate)\n",
        "            raw = data.get(\"prediction\", None)\n",
        "        except Exception:\n",
        "            raw = None\n",
        "\n",
        "    # e) Map intâ†’str or lowercase\n",
        "    if isinstance(raw, int) and int2str:\n",
        "        pred_label = int2str.get(raw)\n",
        "    elif isinstance(raw, str):\n",
        "        pred_label = raw.lower()\n",
        "    else:\n",
        "        pred_label = None\n",
        "\n",
        "    # f) Print first 5 examples\n",
        "    if idx < 5:\n",
        "        print(f\"\\n--- Example {idx} ---\")\n",
        "        print(\"Sentence:  \", sentence)\n",
        "        print(\"True label:\", true_label)\n",
        "        print(\"Raw gen:   \", txt)\n",
        "        print(\"Parsed â†’   \", pred_label)\n",
        "\n",
        "    # g) Compare to true label\n",
        "    if pred_label == true_label:\n",
        "        correct += 1\n",
        "\n",
        "# 4) Report\n",
        "elapsed = time.time() - start\n",
        "accuracy = correct / len(split[\"test\"]) * 100\n",
        "print(f\"\\nEvaluated {len(split['test'])} examples in {elapsed:.1f}s\")\n",
        "print(f\"Accuracy: {accuracy:.2f}% ({correct}/{len(split['test'])})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTDV-WoXLPrQ"
      },
      "outputs": [],
      "source": [
        "# Save the adapter-enhanced model and tokenizer\n",
        "save_path = \"spanglish_sentiment_model\"\n",
        "trainer.save_model(save_path)         # saves the model with LoRA adapters\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model saved to {save_path}/\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
